# Semantic Segmentation on the Mapillary Dataset

Semantic segmentation of streetâ€level imagery using DeepLabV3 and Uâ€‘Net architectures on the Mapillary Vistas dataset.

---

## ğŸ“‹ Table of Contents

- [Overview](##Overview)
- [Repository Structure](#repository-structure)
- [Installation](#installation)
- [Dataset Preparation and Visualization](#dataset-preparation)
- [Training](#training)
- [Model Architectures](#model-architectures)
- [Utilities](#utilities)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

---

## ğŸš€ Overview

This repository provides endâ€‘toâ€‘end code for semantic segmentation of urban scenes using the Mapillary Vistas dataset. It includes implementations of:

- **DeepLabV3** with ResNet backbone and ASPP module
- **Uâ€‘Net** model for semantic segmentation

You can train, evaluate, and run inference on your own images, and compute metrics such as Dice score. You can also experiment with Wandb.

I have written a comprehensive report of the architectures used and the result of the training and evaluation. Please, visit the published article here: [My journey into Semantic Segmentation](https://medium.com/@uzohchinedu/deep-learning-for-computer-vision-my-journey-into-semantic-segmentation-45b9d0491d0f).  

<!-- ## âœ¨ Features

- Dataset loader for Mapillary Vistas (`mapillary_dataset.py`)
- Pretrained ResNet backbone support
- DeepLabV3 and Uâ€‘Net model implementations
- Training loop with customizable losses and metrics
- Evaluation scripts to compute Dice and other segmentation metrics -->

## ğŸ—‚ï¸ Repository Structure

```
mapillary-semantic-segmentation/
â”œâ”€â”€ data                               # Create this folder with Mapillary data, download from https://www.mapillary.com/dataset/vistas
â”œâ”€â”€ notebooks/                         # Jupyter notebooks for analysis and visualization
â”‚   â”œâ”€â”€ basic_stats.ipynb              # Exploratory statistics notebook to obtain image class distributions
â”‚   â”œâ”€â”€ class_distribution_v2.0.json   # Class distribution output data
â”‚   â”œâ”€â”€ deeplab_test.ipynb             # Notebook for testing DeepLab model
â”‚   â””â”€â”€ visualize.ipynb                # Notebook for visualizing predictions in the case of UNET
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â””â”€â”€ mapillary_dataset.py      # Data loader for Mapillary data
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ deeplabv3_model/           # Model implementations
â”‚   â”‚       â”œâ”€â”€ pretrained_resnet/     # Pretrained ResNet weights
â”‚   â”‚       â”‚   â”œâ”€â”€ resnet18-5c106cde.pth
â”‚   â”‚       â”‚   â”œâ”€â”€ resnet34-333f7ec4.pth
â”‚   â”‚       â”‚   â””â”€â”€ resnet50-19c8e357.pth
â”‚   â”‚       â”‚  
â”‚   â”‚       â”œâ”€â”€ aspp.py                # Atrous Spatial Pyramid Pooling
â”‚   â”‚       â”œâ”€â”€ deeplabv3.py           # DeepLabV3 model definition. Note! pretrained weight is called here. 
â”‚   â”‚       â”œâ”€â”€ resnet.py              # ResNet backbone definition
â”‚   â”‚       â”œâ”€â”€ unet_model.py          # U-Net model definition
â”‚   â”‚       â””â”€â”€ unet_parts.py          # U-Net encoder/decoder blocks
â”‚   â”‚      
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ dice_score.py             # Dice coefficient implementation
â”‚       â”œâ”€â”€ losses_and_metrics.py     # Loss functions and metrics
â”‚       â”œâ”€â”€ evaluate.py               # Evaluation script
â”‚       â””â”€â”€ train.py                  # Training script, main file
â””â”€â”€ README.md                         # (this file)
```

## âš™ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/mapillar-semantic-segmentation.git
   cd mapillary-semantic-segmentation/src
   ```

2. Create a Python environment (recommended):
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. Install dependencies:
   ```bash
   pip install \
   torch \  # PyTorch: tensor operations & deep learning framework
   torchvision \  # Torchvision: datasets, models, and transforms for computer vision
   matplotlib \  # Matplotlib: plotting library for creating static visualizations
   jupyter \  # Jupyter: interactive notebook environment
   albumentations \  # Albumentations: fast image augmentation for images
   plotly \  # Plotly: interactive, web-based visualizations
   pillow \  # Pillow: image processing library
   numpy \  # NumPy: fundamental package for numerical computing
   tqdm      # TQDM: progress bars for iterables
   ```


## ğŸ—„ï¸ Dataset Preparation and Visualization

1. **Download** the Mapillary Vistas dataset following the [official instructions](https://www.mapillary.com/dataset/vistas).
2. **Inspect and run** the notebook `basic_stats.ipynb` to obtain basic statatics like the size of the images and the class distribution.
     

## ğŸ‹ï¸ Training

Train a deeplabv3 or unet model :
```bash
python train.py \
  --model deeplabv3 \
  --batch_size 4 \
  --epochs 20 \
```

- Default loss: CrossEntropy + Dice loss (see `losses_and_metrics.py`)
- Checkpoints and logs will be saved under `--output_dir`.



Results will be printed to console and saved as JSON in the checkpoint folder.

## ğŸ§© Model Architectures

- **DeepLabV3**: Uses a ResNet encoder and ASPP module for multiâ€‘scale context aggregation.
- **Uâ€‘Net**: Classic encoderâ€‘decoder with skip connections.

Model code lives under `models/deeplabv3_model/` and at the root of `models/` for Uâ€‘Net.

## ğŸ”§ Utilities

- **`dice_score.py`**: Compute Dice coefficient.
- **`losses_and_metrics.py`**: Builtâ€‘in loss functions (CrossEntropy, Dice, Focal) and metrics.
- **`train.py`**: Endâ€‘toâ€‘end training loop with logging and wandb experimentation.
- **`evaluate.py`**: Compute and report metrics on a dataset.

## ğŸ¤ Contributing

Contributions are welcome! 

## ğŸ“œ License

This project is released under the MIT License. See [LICENSE](LICENSE) for details.

## âœ‰ï¸ Contact

Feel free to open issues or reach out with questions and suggestions!
